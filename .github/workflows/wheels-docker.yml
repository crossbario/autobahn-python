name: wheels-docker

on:
  # Build wheels on feature branches and PRs (test only)
  push:
    branches: ["**"]
  pull_request:
    branches: [master]

  # Publish to GitHub Releases when merged to master
  # Publish to PyPI when tagged
  workflow_dispatch:

env:
  # Registry for caching build images
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/wheel-builder

jobs:
  identifiers:
    # GitHub needs to know where .cicd/workflows/identifiers.yml lives at parse time,
    # and submodules aren't included in that context! thus the following does NOT work:
    # uses: ./.cicd/workflows/identifiers.yml
    # we MUST reference the remote repo directly:
    uses: wamp-proto/wamp-cicd/.github/workflows/identifiers.yml@main
    # IMPORTANT: we still need .cicd as a Git submodule in the using repo though!
    # because e.g. identifiers.yml wants to access scripts/sanitize.sh !

  build-wheels:
    name: Build wheels (${{ matrix.target.name }})
    needs: identifiers
    runs-on: ubuntu-latest
    container: ${{ matrix.target.base_image }}

    env:
      BASE_REPO: ${{ needs.identifiers.outputs.base_repo }}
      BASE_BRANCH: ${{ needs.identifiers.outputs.base_branch }}
      PR_NUMBER: ${{ needs.identifiers.outputs.pr_number }}
      PR_REPO: ${{ needs.identifiers.outputs.pr_repo }}
      PR_BRANCH: ${{ needs.identifiers.outputs.pr_branch }}

    strategy:
      fail-fast: false
      matrix:
        target:
          # manylinux_2_34 (glibc 2.34+) - PEP 600 compliant for modern Linux
          # see: https://github.com/pypa/manylinux
          - name: "manylinux_2_34_x86_64"
            base_image: "quay.io/pypa/manylinux_2_34_x86_64"

          # Future manylinux images can be added here:
          # - name: "manylinux_2_34_aarch64"
          #   base_image: "quay.io/pypa/manylinux_2_34_aarch64"

          # Deactivated for now - focusing on standard manylinux wheels:
          # - name: "debian12-amd64"
          #   base_image: "debian:12"
          # - name: "rocky9-amd64"
          #   base_image: "rockylinux:9"
          # - name: "ubuntu2404-amd64"
          #   base_image: "ubuntu:24.04"

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Install system dependencies
        run: |
          # manylinux images use yum/dnf and come with many build tools pre-installed
          if command -v yum >/dev/null 2>&1; then
            # manylinux images (CentOS-based)
            yum update -y
            yum install -y \
              curl \
              git \
              openssl-devel \
              libffi-devel \
              zlib-devel \
              bzip2-devel \
              readline-devel \
              sqlite-devel \
              ncurses-devel
            # Note: snappy-devel may not be available in manylinux base images
            yum install -y snappy-devel || echo "snappy-devel not available, skipping"
          elif command -v dnf >/dev/null 2>&1; then
            # Newer manylinux images might use dnf
            dnf update -y
            dnf install -y \
              curl \
              git \
              openssl-devel \
              libffi-devel \
              zlib-devel \
              bzip2-devel \
              readline-devel \
              sqlite-devel \
              ncurses-devel
            dnf install -y snappy-devel || echo "snappy-devel not available, skipping"
          fi

      - name: Setup Python environment
        run: |
          # manylinux images come with multiple Python versions pre-installed in /opt/python/
          echo "==> Available Python versions:"
          ls -la /opt/python/*/bin/python* 2>/dev/null || echo "No /opt/python found"

          # Add all Python versions to PATH for uv/just to discover
          for pyver in /opt/python/*/bin; do
            if [ -d "$pyver" ]; then
              echo "Adding $pyver to PATH"
              export PATH="$pyver:$PATH"
            fi
          done

          # Also ensure we have a working python3 symlink
          which python3 || ln -sf $(find /opt/python -name python3 | head -1) /usr/local/bin/python3

          echo ""
          echo "==> Current Python version:"
          python3 --version
          echo "==> pip version:"
          python3 -m pip --version

          # Save the updated PATH for subsequent steps
          echo "PATH=$PATH" >> $GITHUB_ENV

      - name: Install Just
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          curl --proto '=https' --tlsv1.2 -sSf https://just.systems/install.sh | bash -s -- --to /usr/local/bin
          just --version

      - name: Install uv
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          # Add to both GITHUB_PATH and current environment
          echo "/root/.cargo/bin" >> $GITHUB_PATH
          echo "PATH=/root/.cargo/bin:$PATH" >> $GITHUB_ENV
          export PATH="/root/.cargo/bin:$PATH"
          uv --version

      - name: Install Rust
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --default-toolchain stable
          # Source the Rust environment (as recommended by installer)
          source "$HOME/.cargo/env"
          # Add to both GITHUB_PATH and current environment
          echo "/root/.cargo/bin" >> $GITHUB_PATH
          echo "PATH=/root/.cargo/bin:$PATH" >> $GITHUB_ENV
          export PATH="/root/.cargo/bin:$PATH"
          rustc --version

      - name: Verify toolchain
        run: |
          # Source Rust environment and ensure PATH is set
          source "$HOME/.cargo/env" 2>/dev/null || true
          export PATH="/root/.cargo/bin:$PATH"
          echo "==> Build environment summary:"
          echo "Container: ${{ matrix.target.base_image }}"
          echo "Just: $(just --version)"
          echo "uv: $(uv --version)"
          echo "Rust: $(rustc --version)"
          echo "Python: $(python3 --version)"
          echo "GCC: $(gcc --version | head -1)"
          echo "glibc: $(ldd --version 2>/dev/null | head -1 || echo 'N/A')"

      - name: Build manylinux wheels with NVX native extension
        env:
          AUTOBAHN_USE_NVX: 1
        run: |
          # Source Rust environment and ensure tools are in PATH
          source "$HOME/.cargo/env" 2>/dev/null || true
          export PATH="/root/.cargo/bin:$PATH"

          # Verify environment
          echo "==> Environment verification:"
          echo "AUTOBAHN_USE_NVX=$AUTOBAHN_USE_NVX"
          echo "PATH=$PATH"
          echo "Python: $(python3 --version)"
          echo "uv: $(uv --version)"
          echo "just: $(just --version)"
          echo "rustc: $(rustc --version)"
          echo "auditwheel: $(auditwheel --version || echo 'not available')"

          # Build binary wheels WITH NVX acceleration for manylinux
          just build-all

          command -v auditwheel >/dev/null || { echo "auditwheel missing, aborting"; exit 1; }
          mkdir -p wheelhouse

          # Convert linux_x86_64 wheels to multi-platform tag (incl. manylinux_2_34_x86_64) using auditwheel
          echo ""
          echo "==> Converting wheels to multi-platform tag (incl. manylinux_2_34_x86_64) format..."
          for wheel in dist/*.whl; do
            if [[ "$wheel" == *"linux_x86_64"* ]]; then
              echo "Converting: $(basename $wheel)"

              # show autodetected set of platform tags
              auditwheel show "$wheel"

              # fix/rename wheel based on autodetected platform tags and store in wheelhouse/
              # -> auditwheel will encode all (!) supported tags into the wheel filename
              # -> Python packaging ecosystem allows wheels to carry multiple platform tags
              # -> pip will then pick the most specific tag that works for the current system
              # -> so we do not need to manually set: --plat manylinux_2_34_x86_64 as in:
              #     auditwheel repair "$wheel" --plat manylinux_2_34_x86_64 -w wheelhouse/
              auditwheel repair "$wheel" -w wheelhouse/
            else
              echo "Copying non-linux wheel: $(basename $wheel)"
              cp "$wheel" wheelhouse/
            fi
          done
          echo "Copying source distribution"
          cp dist/*.tar.gz wheelhouse/

          echo ""
          echo "==> Final wheel inventory after manylinux conversion:"
          echo ""
          ls -la wheelhouse/
          for wheel in wheelhouse/*.whl; do
            auditwheel show "$wheel"
          done
          echo ""
          echo "    Note: auditwheel adds multiple manylinux tags into the filename."
          echo "          This is expected and pip will resolve correctly."
          echo "          Do NOT try to simplify by renaming."
          echo ""

      - name: Generate build metadata
        run: |
          BUILD_INFO=wheelhouse/build-info.txt

          echo "manylinux Build Information for ${{ matrix.target.name }}" > $BUILD_INFO
          echo "========================================================" >> $BUILD_INFO
          echo "" >> $BUILD_INFO

          echo "Build Date: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> $BUILD_INFO
          echo "Container: ${{ matrix.target.base_image }}" >> $BUILD_INFO
          echo "Platform: $(uname -m)" >> $BUILD_INFO
          echo "Build Method: GitHub Actions + manylinux Docker Container" >> $BUILD_INFO
          echo "NVX Acceleration: ENABLED (binary wheels with native extensions)" >> $BUILD_INFO
          echo "" >> $BUILD_INFO

          echo "Per-Wheel ABI / Platform Tags:" >> $BUILD_INFO
          echo "------------------------------" >> $BUILD_INFO
          for whl in wheelhouse/*.whl; do
            echo "- $(basename "$whl")" >> $BUILD_INFO
            auditwheel show "$whl" | grep "platform tag" | awk -F'"' '{print "  * " $2}' >> $BUILD_INFO
            echo "" >> $BUILD_INFO
          done

          echo "" >> $BUILD_INFO
          echo "Global System Information:" >> $BUILD_INFO
          echo "--------------------------" >> $BUILD_INFO
          echo "- OS: $(cat /etc/os-release | grep PRETTY_NAME | cut -d'\"' -f2 2>/dev/null || echo 'manylinux container')" >> $BUILD_INFO
          echo "- Kernel: $(uname -r)" >> $BUILD_INFO
          echo "- glibc: $(ldd --version 2>/dev/null | head -1 || echo 'N/A')" >> $BUILD_INFO
          echo "- Architecture: $(uname -m)" >> $BUILD_INFO
          echo "" >> $BUILD_INFO

          echo "Build Tools:" >> $BUILD_INFO
          echo "------------" >> $BUILD_INFO
          echo "- Just: $(just --version)" >> $BUILD_INFO
          echo "- uv: $(uv --version)" >> $BUILD_INFO
          echo "- Rust: $(rustc --version)" >> $BUILD_INFO
          echo "- Python: $(python3 --version)" >> $BUILD_INFO
          echo "- GCC: $(gcc --version | head -1)" >> $BUILD_INFO

          echo ""
          echo "==> Generated build-info.txt:"
          cat $BUILD_INFO

      - name: List built artifacts
        run: |
          echo "==> Built artifacts for ${{ matrix.target.name }}:"
          ls -la wheelhouse/ 2>/dev/null || echo "No wheelhouse/ directory found"

          echo ""
          echo "==> Build metadata:"
          cat wheelhouse/build-info.txt 2>/dev/null || echo "No build info found"

          echo ""
          echo "==> Wheel inventory:"
          find wheelhouse/ -name "*.whl" -exec basename {} \; 2>/dev/null | sort || echo "No wheels found"

      - name:
          Upload wheels, source dist and build metadata artifacts
        uses: actions/upload-artifact@v4
        with:
          name: artifacts-${{ matrix.target.name }}
          path: |
            wheelhouse/*.whl
            wheelhouse/*.tar.gz
            wheelhouse/build-info.txt
          retention-days: 30

  # GitHub Releases, PyPI, and RTD publishing are now handled by the centralized 'release' workflow
